{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2286d785",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-10T01:01:46.995473Z",
     "iopub.status.busy": "2021-09-10T01:01:46.995473Z",
     "iopub.status.idle": "2021-09-10T01:01:47.880473Z",
     "shell.execute_reply": "2021-09-10T01:01:47.879471Z",
     "shell.execute_reply.started": "2021-09-10T01:01:46.995473Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.6\n"
     ]
    }
   ],
   "source": [
    "# run in prompt -> python -m pip install findspark\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark\n",
    "findspark.find()\n",
    "print(pyspark.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e4e949b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-10T01:01:56.416219Z",
     "iopub.status.busy": "2021-09-10T01:01:56.416219Z",
     "iopub.status.idle": "2021-09-10T01:02:16.641299Z",
     "shell.execute_reply": "2021-09-10T01:02:16.640296Z",
     "shell.execute_reply.started": "2021-09-10T01:01:56.416219Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.6</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>TCC_covid</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1c156828a58>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf, SparkFiles\n",
    "import urllib.request\n",
    "from datetime import date\n",
    "\n",
    "\n",
    "appName = \"TCC_covid\"\n",
    "# master = \"local\"\n",
    "# conf = SparkConf() \\\n",
    "#     .setAppName(appName)\n",
    "\n",
    "# sc = SparkContext(conf=conf)\n",
    "# spark = SparkSession(sc) \\\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(appName) \\\n",
    "        .master('local[*]')\\\n",
    "        .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.11:2.2.0\")\\\n",
    "        .config(\"spark.mongodb.input.uri\", \"mongodb://localhost:27017/teste.coll\") \\\n",
    "        .config(\"spark.mongodb.output.uri\", \"mongodb://localhost:27017/teste.coll\") \\\n",
    "        .getOrCreate()\n",
    "# .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\") \\\n",
    "# .config(\"spark.app.id\", \"MongoSparkConnectorTour\") \\\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd0d7e0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-10T01:12:21.383515Z",
     "iopub.status.busy": "2021-09-10T01:12:21.382547Z",
     "iopub.status.idle": "2021-09-10T01:12:21.915429Z",
     "shell.execute_reply": "2021-09-10T01:12:21.914428Z",
     "shell.execute_reply.started": "2021-09-10T01:12:21.382547Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o179.addFile.\n: org.apache.spark.SparkException: File C:\\Users\\ferna\\AppData\\Local\\Temp\\spark-77f8ec1f-9907-4e4f-bef6-09d51ecfb611\\userFiles-ab2b2c0d-0e3c-4b96-b1ac-12182aba9840\\sp.csv exists and does not match contents of https://github.com/seade-R/dados-covid-sp/blob/c8e4ccfea61a47b5483b1c8b81cfdc287c64403d/data/sp.csv\r\n\tat org.apache.spark.util.Utils$.copyFile(Utils.scala:614)\r\n\tat org.apache.spark.util.Utils$.downloadFile(Utils.scala:566)\r\n\tat org.apache.spark.util.Utils$.doFetchFile(Utils.scala:714)\r\n\tat org.apache.spark.util.Utils$.fetchFile(Utils.scala:509)\r\n\tat org.apache.spark.SparkContext.addFile(SparkContext.scala:1568)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-46383e8b9a58>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# text = data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSparkFiles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"sp.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minferSchema\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m';'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ArquivosDeProgramas\\spark-2.4.6-bin-hadoop2.6\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36maddFile\u001b[1;34m(self, path, recursive)\u001b[0m\n\u001b[0;32m    914\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m400\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    915\u001b[0m         \"\"\"\n\u001b[1;32m--> 916\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    917\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    918\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0maddPyFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ArquivosDeProgramas\\spark-2.4.6-bin-hadoop2.6\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ArquivosDeProgramas\\spark-2.4.6-bin-hadoop2.6\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ArquivosDeProgramas\\spark-2.4.6-bin-hadoop2.6\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o179.addFile.\n: org.apache.spark.SparkException: File C:\\Users\\ferna\\AppData\\Local\\Temp\\spark-77f8ec1f-9907-4e4f-bef6-09d51ecfb611\\userFiles-ab2b2c0d-0e3c-4b96-b1ac-12182aba9840\\sp.csv exists and does not match contents of https://github.com/seade-R/dados-covid-sp/blob/c8e4ccfea61a47b5483b1c8b81cfdc287c64403d/data/sp.csv\r\n\tat org.apache.spark.util.Utils$.copyFile(Utils.scala:614)\r\n\tat org.apache.spark.util.Utils$.downloadFile(Utils.scala:566)\r\n\tat org.apache.spark.util.Utils$.doFetchFile(Utils.scala:714)\r\n\tat org.apache.spark.util.Utils$.fetchFile(Utils.scala:509)\r\n\tat org.apache.spark.SparkContext.addFile(SparkContext.scala:1568)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n"
     ]
    }
   ],
   "source": [
    "url = \"https://github.com/seade-R/dados-covid-sp/blob/c8e4ccfea61a47b5483b1c8b81cfdc287c64403d/data/sp.csv\"\n",
    "\n",
    "# ano = url[44:48]\n",
    "# mes = url[49:51]\n",
    "\n",
    "# dataAtual = date.today()\n",
    "# diaAtual = str(dataAtual)[8:10]\n",
    "# mesAtual = str(dataAtual)[5:7]\n",
    "# anoAtual = str(dataAtual)[0:4]\n",
    "\n",
    "# if int(diaAtual) > 1 and int(mesAtual) > int(mes):\n",
    "#     url = url.replace(mes, mesAtual)\n",
    "\n",
    "# if int(anoAtual) > int(ano) and int(diaAtual) > 1:\n",
    "#     url = url.replace(ano, anoAtual)\n",
    "\n",
    "# response = urllib.request.urlopen(url)\n",
    "# data = response.read()\n",
    "# text = data\n",
    "\n",
    "spark.sparkContext.addFile(url)\n",
    "\n",
    "df = spark.read.csv(SparkFiles.get(\"sp.csv\"), inferSchema=True, header=True, sep=';')\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "df = df.na.fill(0)\n",
    "\n",
    "df.show(20,False)\n",
    "\n",
    "# df = df.withColumn('Data', regexp_replace('Data', 'jan', '01'))\n",
    "# df = df.withColumn('Data', regexp_replace('Data', 'fev', '02'))\n",
    "# df = df.withColumn('Data', regexp_replace('Data', 'mar', '03'))\n",
    "# df = df.withColumn('Data', regexp_replace('Data', 'abr', '04'))\n",
    "# df = df.withColumn('Data', regexp_replace('Data', 'mai', '05'))\n",
    "# df = df.withColumn('Data', regexp_replace('Data', 'jun', '06'))\n",
    "# df = df.withColumn('Data', regexp_replace('Data', 'jul', '07'))\n",
    "# df = df.withColumn('Data', regexp_replace('Data', 'ago', '08'))\n",
    "# df = df.withColumn('Data', regexp_replace('Data', 'set', '09'))\n",
    "# df = df.withColumn('Data', regexp_replace('Data', 'out', '10'))\n",
    "# df = df.withColumn('Data', regexp_replace('Data', 'nov', '11'))\n",
    "# df = df.withColumn('Data', regexp_replace('Data', 'dez', '12'))\n",
    "\n",
    "# df = df.withColumn('Obitos_por_Dia', lit(col('�bitos por dia')))\n",
    "\n",
    "# df = df.withColumn('Data', from_unixtime(unix_timestamp('Data', 'dd/MM/yyyy')))\n",
    "\n",
    "# df.select(substring(df.Data,1,10).alias('Data'), 'Obitos_por_Dia').orderBy(col('Data').desc()).show(10,truncate=False)\n",
    "\n",
    "# df.select(format_number(sum('Casos por dia'),0).alias('Total_de_Casos'), format_number(sum('Obitos_por_Dia'),0).alias('Total_de_Mortes')).show()\n",
    "\n",
    "# df.write.format(\"com.mongodb.spark.sql.DefaultSource\").mode(\"overwrite\").save()\n",
    "# # df.write.format(\"mongo\").mode(\"append\").save()\n",
    "\n",
    "# df2 = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").load()\n",
    "\n",
    "# df2.show(20,False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
